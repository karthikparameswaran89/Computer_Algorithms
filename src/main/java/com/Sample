import pandas as pd
from azure.identity import DefaultAzureCredential
from azure.storage.filedatalake import DataLakeServiceClient
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# === CONFIGURATION ===

METADATA_CSV = "metadata.csv"                      # Must contain: catalog, schema, table_name, table_type, table_format, location
OUTPUT_CSV = "all_table_sizes.csv"

STORAGE_ACCOUNT_NAME = "<your-storage-account>"   # without https:// or .dfs.core.windows.net
MAX_WORKERS = 20

# =======================

def extract_filesystem_and_path(abfss_url: str):
    """
    From abfss://<container>@<account>.dfs.core.windows.net/path/to/table
    → returns: <container>, <path/to/table>
    """
    parsed = urlparse(abfss_url)
    if not parsed.netloc or "@" not in parsed.netloc:
        raise ValueError(f"Invalid abfss URL: {abfss_url}")
    
    container = parsed.netloc.split("@")[0]
    path = parsed.path.lstrip("/")
    return container, path

def list_datalake_size(datalake_service_client, abfss_url, catalog, schema, table, table_type, table_format):
    try:
        filesystem_name, dir_path = extract_filesystem_and_path(abfss_url)
        filesystem_client = datalake_service_client.get_file_system_client(filesystem=filesystem_name)

        total_size = 0
        paths = filesystem_client.get_paths(path=dir_path, recursive=True)
        for path in paths:
            if not path.is_directory:
                total_size += path.content_length

        size_mb = round(total_size / (1024 ** 2), 2)
        print(f"✔ {schema}.{table} [{table_type}, {table_format}] — {size_mb} MB")

        return {
            "catalog": catalog,
            "schema": schema,
            "table_name": table,
            "table_type": table_type,
            "table_format": table_format,
            "location": abfss_url,
            "size_in_bytes": total_size,
            "size_in_mb": size_mb,
            "error": None
        }

    except Exception as e:
        print(f"✘ {schema}.{table} — ERROR: {e}")
        return {
            "catalog": catalog,
            "schema": schema,
            "table_name": table,
            "table_type": table_type,
            "table_format": table_format,
            "location": abfss_url,
            "size_in_bytes": -1,
            "size_in_mb": -1,
            "error": str(e)
        }

def main():
    df = pd.read_csv(METADATA_CSV)

    required_cols = {"catalog", "schema", "table_name", "table_type", "table_format", "location"}
    if not required_cols.issubset(set(df.columns)):
        raise ValueError(f"Input CSV must include columns: {', '.join(required_cols)}")

    # Filter out views (no data)
    df = df[~df["table_type"].str.upper().isin(["VIEW"])]
    df = df[df["location"].notnull() & df["location"].str.startswith("abfss://")]

    # Set up DataLake client
    credential = DefaultAzureCredential()
    account_url = f"https://{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net"
    datalake_service_client = DataLakeServiceClient(account_url=account_url, credential=credential)

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(
                list_datalake_size,
                datalake_service_client,
                row["location"],
                row["catalog"],
                row["schema"],
                row["table_name"],
                row["table_type"],
                row["table_format"]
            ): (row["schema"], row["table_name"])
            for _, row in df.iterrows()
        }

        for future in as_completed(futures):
            results.append(future.result())

    df_out = pd.DataFrame(results)
    df_out.to_csv(OUTPUT_CSV, index=False)
    print(f"\n✅ Saved table size report: {OUTPUT_CSV}")

if __name__ == "__main__":
    main()
