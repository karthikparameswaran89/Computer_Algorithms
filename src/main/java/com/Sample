from py4j.java_gateway import java_import
from pyspark.sql import Row

# Import Hadoop classes
java_import(spark._jvm, "org.apache.hadoop.fs.FileSystem")
java_import(spark._jvm, "org.apache.hadoop.fs.Path")

# Function to compute recursive size using Hadoop API
def get_size(path: str) -> int:
    fs = spark._jvm.FileSystem.get(spark._jsc.hadoopConfiguration())
    p = spark._jvm.Path(path)
    try:
        return fs.getContentSummary(p).getLength()
    except Exception as e:
        print(f"Error on {path}: {e}")
        return -1   # mark failed

# Partition-level processor
def process_partition(rows_iter):
    results = []
    for row in rows_iter:
        size = get_size(row.storage_location)
        results.append(Row(table_name=row.table_name, size_bytes=size))
    return iter(results)

# ---- DRIVER CODE ----

# control concurrency: each partition = one task
PARTITIONS = 100   # tune this: fewer partitions = fewer concurrent file ops

# repartition so we donâ€™t launch 10k tasks
df_partitioned = df.repartition(PARTITIONS)

# run distributed job
result_df = df_partitioned.rdd.mapPartitions(process_partition).toDF()

# now you have (table_name, size_bytes)
result_df.show(20, truncate=False)
