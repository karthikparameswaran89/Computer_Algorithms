import os
import pandas as pd
from delta.tables import DeltaTable
from pyspark.sql import SparkSession
from concurrent.futures import ThreadPoolExecutor, as_completed

# Configs
INPUT_CSV = "/dbfs/tmp/metadata.csv"          # Input CSV with table names (and optionally locations)
OUTPUT_CSV = "/dbfs/tmp/delta_table_sizes.csv"  # Output file
MAX_WORKERS = 16                               # Tune this per cluster size

spark = SparkSession.builder.getOrCreate()

def get_table_location(table_name: str) -> str:
    """Gets the physical location of a Delta table"""
    try:
        location = spark.sql(f"DESCRIBE DETAIL {table_name}").select("location").collect()[0][0]
        return location
    except Exception as e:
        raise RuntimeError(f"Failed to get location for {table_name}: {e}")

def get_folder_size(dbfs_path: str) -> int:
    """Recursively compute size of all files under a DBFS path (in bytes)"""
    local_path = "/dbfs" + dbfs_path.replace("dbfs:", "")
    total_size = 0
    for dirpath, _, filenames in os.walk(local_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            try:
                total_size += os.path.getsize(fp)
            except Exception:
                pass  # skip unreadable files
    return total_size

def process_table(table_name: str):
    """Processes a single table to compute full storage size"""
    try:
        location = get_table_location(table_name)
        size_bytes = get_folder_size(location)
        size_gb = round(size_bytes / (1024 ** 3), 3)
        print(f"‚úÖ {table_name} ‚Üí {size_gb} GB")
        return {
            "table": table_name,
            "location": location,
            "size_bytes": size_bytes,
            "size_gb": size_gb,
            "error": None
        }
    except Exception as e:
        print(f"‚ùå {table_name} ‚Üí ERROR: {e}")
        return {
            "table": table_name,
            "location": None,
            "size_bytes": -1,
            "size_gb": -1,
            "error": str(e)
        }

def main():
    # Load table list from CSV
    df = pd.read_csv(INPUT_CSV)
    if "table" not in df.columns:
        raise ValueError("Input CSV must have a 'table' column with full table names (e.g. hive_metastore.default.my_table)")

    table_names = df["table"].dropna().unique().tolist()

    # Run in parallel
    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {executor.submit(process_table, tbl): tbl for tbl in table_names}
        for future in as_completed(futures):
            results.append(future.result())

    # Write to CSV
    df_out = pd.DataFrame(results)
    df_out.to_csv(OUTPUT_CSV, index=False)
    print(f"\nüìÅ Size report saved to: {OUTPUT_CSV}")

# Run it
if __name__ == "__main__":
    main()
