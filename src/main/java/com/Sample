import re
from msal import ConfidentialClientApplication
from databricks import sql
import pandas as pd

# Azure AD credentials
TENANT_ID = "your-tenant-id"
CLIENT_ID = "your-client-id"
CLIENT_SECRET = "your-client-secret"

# Databricks workspace
DATABRICKS_HOSTNAME = "your-databricks-instance.cloud.databricks.com"
DATABRICKS_HTTP_PATH = "your-sql-warehouse-http-path"
DATABRICKS_SCOPE = "2ff814a6-3304-4ab8-85cb-cd0e6f879c1d/.default"

# Metadata columns
METADATA_COLUMNS = [
    "catalog",
    "schema",
    "table_name",
    "table_format",
    "table_type"
]

def get_azure_ad_token():
    app = ConfidentialClientApplication(
        CLIENT_ID,
        authority=f"https://login.microsoftonline.com/{TENANT_ID}",
        client_credential=CLIENT_SECRET,
    )
    res = app.acquire_token_for_client(scopes=[DATABRICKS_SCOPE])
    if "access_token" in res:
        return res["access_token"]
    raise Exception(f"Failed to acquire token: {res.get('error_description')}")

def get_databricks_connection():
    token = get_azure_ad_token()
    return sql.connect(
        server_hostname=DATABRICKS_HOSTNAME,
        http_path=DATABRICKS_HTTP_PATH,
        access_token=token
    )

def get_schemas(cursor, catalog="hive_metastore"):
    cursor.execute(f"SHOW SCHEMAS IN `{catalog}`")
    return [r[0] for r in cursor.fetchall()]

def get_tables_with_extended(cursor, catalog, schema):
    cursor.execute(f"SHOW TABLE EXTENDED IN `{catalog}`.`{schema}` LIKE '*'")
    return cursor.fetchall()

def parse_table_extended(row):
    # row: (database, tableName, isTemporary, detailed string)
    _, table_name, _, detail = row
    table_format = None
    table_type = None
    if detail:
        # Try to extract provider and type using regex
        format_match = re.search(r"Provider: (\w+)", detail)
        type_match = re.search(r"Type: (\w+)", detail)
        table_format = format_match.group(1) if format_match else ""
        table_type = type_match.group(1) if type_match else ""
    return table_name, table_format, table_type

def collect_metadata(catalog="hive_metastore"):
    records = []
    with get_databricks_connection() as conn:
        cursor = conn.cursor()
        schemas = get_schemas(cursor, catalog)

        for schema in schemas:
            print(f"Scanning schema: {schema}")
            try:
                rows = get_tables_with_extended(cursor, catalog, schema)
                for row in rows:
                    table_name, fmt, ttype = parse_table_extended(row)
                    records.append({
                        "catalog": catalog,
                        "schema": schema,
                        "table_name": table_name,
                        "table_format": fmt,
                        "table_type": ttype
                    })
            except Exception as e:
                print(f"  [!] Failed to scan schema {schema}: {e}")
        cursor.close()
    return records

def save_to_csv(records, output="metadata.csv"):
    df = pd.DataFrame(records, columns=METADATA_COLUMNS)
    df.to_csv(output, index=False)
    print(f"âœ… Metadata saved to: {output}")
    return df

if __name__ == "__main__":
    catalog = "hive_metastore"
    metadata = collect_metadata(catalog)
    df = save_to_csv(metadata)
    print(df.head())
