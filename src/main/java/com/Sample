from pyspark.sql import Row

# ---- Config ----
BATCH_SIZE = 1000        # how many rows per batch
PARTITIONS_PER_BATCH = 50  # how many concurrent tasks allowed per batch

# ---- Recursive size function ----
def get_size(path: str) -> int:
    total_size = 0
    try:
        files = dbutils.fs.ls(path)
        for f in files:
            if f.isDir():
                total_size += get_size(f.path)  # recursion
            else:
                total_size += f.size
    except Exception as e:
        print(f"Error on {path}: {e}")
    return total_size

# ---- Partition processor ----
def process_partition(rows_iter):
    results = []
    for row in rows_iter:
        size = get_size(row.storage_location)
        results.append(Row(table_name=row.table_name, size_bytes=size))
    return iter(results)

# ---- Driver: batch loop ----
rows = df.collect()   # careful: ensure row count is manageable (10k fine)

results_all = []

for i in range(0, len(rows), BATCH_SIZE):
    batch = rows[i:i+BATCH_SIZE]

    # convert this batch back to DF
    batch_df = spark.createDataFrame(batch)

    # control #concurrent tasks
    batch_df = batch_df.repartition(PARTITIONS_PER_BATCH)

    # run distributed job for this batch
    result_df = batch_df.rdd.mapPartitions(process_partition).toDF()

    # collect or append to list
    batch_results = result_df.collect()
    results_all.extend(batch_results)

# ---- Final results as DF ----
final_df = spark.createDataFrame(results_all)
final_df.show()
