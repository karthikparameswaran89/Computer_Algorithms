from py4j.java_gateway import java_import
from pyspark.sql import Row

# Config
BATCH_SIZE = 1000
PARTITIONS = 50  # tasks per batch

# ---- Physical size function (safe on executors) ----
def get_physical_size(path, jvm):
    fs = jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
    try:
        p = jvm.org.apache.hadoop.fs.Path(path)
        return fs.getContentSummary(p).getLength()
    except:
        return -1

# ---- Process a batch on driver for active size ----
def get_active_sizes(batch):
    active_sizes = []
    for row in batch:
        try:
            active_files = spark.read.format("delta").load(row.storage_location)._jdf.inputFiles()
            total = 0
            for f in active_files:
                total += dbutils.fs.ls(f)[0].size  # simple alternative
            active_sizes.append(total)
        except:
            active_sizes.append(-1)
    return active_sizes

# ---- DRIVER: batching loop ----
rows = df.collect()  # bring to driver for batch processing
results = []

for i in range(0, len(rows), BATCH_SIZE):
    batch = rows[i:i+BATCH_SIZE]

    # ---- Physical sizes via executor tasks ----
    from pyspark.sql import SparkSession
    jvm = spark._jvm
    batch_df = spark.createDataFrame(batch).repartition(PARTITIONS)

    def partition_phys_size(rows_iter):
        local_jvm = spark._jvm
        res = []
        for row in rows_iter:
            size = get_physical_size(row.storage_location, local_jvm)
            res.append(Row(table_name=row.table_name, physical_size=size))
        return iter(res)

    phys_df = batch_df.rdd.mapPartitions(partition_phys_size).toDF()
    phys_sizes = phys_df.collect()

    # ---- Active sizes on driver ----
    active_sizes = get_active_sizes(batch)

    # ---- Combine results ----
    for idx, row in enumerate(batch):
        results.append(Row(
            table_name=row.table_name,
            physical_size_bytes=phys_sizes[idx].physical_size,
            active_size_bytes=active_sizes[idx]
        ))

# ---- Final DataFrame ----
final_df = spark.createDataFrame(results)
final_df.show(20, truncate=False)
