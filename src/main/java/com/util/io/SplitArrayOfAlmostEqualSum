import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.functions._

def splitArray(df: DataFrame, n: Int): Array[Array[Int]] = {
  val window = Window.orderBy("value").rowsBetween(Window.unboundedPreceding, Window.currentRow)
  val subarrays = df
    .withColumn("subarray", row_number.over(window) % n)
    .groupBy("subarray")
    .agg(collect_list("value").alias("subarray_values"))
    .orderBy("subarray")
    .select("subarray_values")
    .collect()
    .map(row => row.getAs[Seq[Int]]("subarray_values").toArray)
  subarrays
}

val arr = Array.range(1, 100)
val n = 4
val df = arr.toSeq.toDF("value")
val subarrays = splitArray(df, n)
println(subarrays.map(_.mkString(", ")).mkString("\n"))
