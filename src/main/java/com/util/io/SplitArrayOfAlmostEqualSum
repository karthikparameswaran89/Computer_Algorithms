import org.apache.spark.sql.functions._

def splitArray(df: DataFrame, n: Int): Array[Array[(Int, String)]] = {
  val cumulativeSum = df
    .withColumn("cum_sum", sum("value").over(Window.orderBy("id")))
    .select("id", "value", "cum_sum")
  
  val subarrays = Array.fill(n)(Array.empty[(Int, String)])
  val sortedRows = cumulativeSum.sort(desc("value"))
  
  sortedRows.collect().foreach(row => {
    val idx = subarrays.zipWithIndex.minBy { case (subarray, _) => subarray.map(_._2).sum }._2
    subarrays(idx) :+= (row.getAs[Int]("value"), row.getAs[String]("id"))
  })

  subarrays
}

// Example usage
val arr = Seq((1, "a"), (2, "b"), (3, "c"), (4, "d"), (5, "e"), (6, "f"), (7, "g"), (8, "h"), (9, "i"), (10, "j"))
val n = 4
val df = arr.toDF("value", "id")
val subarrays = splitArray(df, n)
subarrays.foreach(subarray => println(subarray.map(_._2).mkString(", ")))
