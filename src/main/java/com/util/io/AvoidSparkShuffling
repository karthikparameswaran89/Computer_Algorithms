import org.apache.spark.sql.Row

def distinctPartition(iterator: Iterator[Row], colNames: List[String]): Iterator[Row] = {
  var seen = Set[List[Any]]()
  var result = List[Row]()

  while (iterator.hasNext) {
    val row = iterator.next()
    val key = colNames.map(c => row.getAs(c))

    if (!seen.contains(key)) {
      seen += key
      result ::= Row.fromSeq(row.toSeq)
    }
  }

  result.iterator
}

val distinctCols = List("col1", "col2", "col3")
val distinctDF = df.rdd.mapPartitions(iter => distinctPartition(iter, distinctCols)).toDF()
