import org.apache.spark.sql.functions._
import org.apache.spark.sql.{DataFrame, Row, SparkSession, Window}

val distinctCols = df.columns
val windowSpec = Window.partitionBy(distinctCols.map(col): _*)

val distinctDF = df.mapPartitions(rows => {
  val seen = scala.collection.mutable.Set.empty[Seq[Any]]
  rows.filter(row => {
    val key = distinctCols.map(row.get)
    if (seen.contains(key)) {
      false
    } else {
      seen += key
      true
    }
  })
})(spark.implicits.rowEncoder(df.schema))
  .withColumn("row_num", row_number().over(windowSpec))
  .filter($"row_num" === 1)
  .drop("row_num")
