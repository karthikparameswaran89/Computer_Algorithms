import org.apache.spark.sql.functions._
import org.apache.spark.sql.{DataFrame, SparkSession, Row}

val spark = SparkSession.builder().getOrCreate()

// Assuming Column_1 is the partitioned column
val partitionColumn = "Column_1"

val distinctCols = df.columns.toSeq
val windowSpec = Window.partitionBy(distinctCols.map(col): _*)

val distinctDF = df
  .repartition(col(partitionColumn)) // Repartition by the partition column
  .sortWithinPartitions(col(partitionColumn)) // Sort within each partition
  .mapPartitions(rows => {
    val seen = scala.collection.mutable.Set.empty[Seq[Any]]
    rows.filter(row => {
      val key = distinctCols.map(row.get)
      if (seen.contains(key)) {
        false
      } else {
        seen += key
        true
      }
    })
  })(org.apache.spark.sql.Encoders.tuple(Row.fromSeq(df.schema.map(_.dataType)): _*))
  .withColumn("row_num", row_number().over(windowSpec))
  .filter($"row_num" === 1)
  .drop("row_num")
