import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window

// Assuming you have a DataFrame named 'df' that is already partitioned by the 'column_name' column
val distinctCols = Seq("column_name_1", "column_name_2", "column_name_3")

val distinctDf = df
  .select(distinctCols.head, distinctCols.tail:_*) // Select the columns to perform distinct on
  .withColumn("rank", row_number().over(Window.partitionBy(distinctCols.map(col(_)): _*).orderBy(monotonically_increasing_id()))) // Add a unique rank to each row within its partition
  .filter(col("rank") === 1) // Filter out all but the first row in each partition

// Cache the resulting DataFrame to avoid recomputation
distinctDf.cache()
